\section{Memory Object Caches}

Traditionally, a cache is a data structure in either hardware or software capable of storage and retrieval of data. Generally, a \textit{value} of a computation is stored in the data structure with a given \textit{key}. A cache is generally used to speed up data retrieval. Often, a pattern of execution is to first attempt to retrieve a \textit{value} from the cache by it's \textit{key}. If the \textit{key} is present in the cache, \textit{value} is returned which is called a \textit{hit}. Otherwise, a failed retrieval is indicated and the attempt to access the cache is called a \textit{miss}. If a miss occurs, data is frequently computed or retrieved elsewhere and stored in the cache to speed up the next execution cycle.

Caches are a heavily used across hardware and software systems. For example, the CPU uses multiple levels of caches in order to speed up memory access. Another example of a cache is in database servers to cache queries and reduce computation time. Efficient use of caching can drastically improve time required to retrieve data.


\subsection{Purpose}
Firstly, the purpose of a memory object cache is to use the machine's available RAM for key-value storage. The implication of a \textit{memory object cache} is that data is only stored in memory and should not be offloaded on the hardware in order to not incur hard drive retrieval delay. As a result, memory caches are often explicitly configured with the maximum amount of memory available.

Secondly, an \textit{object} cache implies that the cache itself is not concerned with the type of data (binary, text) stored within. As a result, memory object caches are multi-purpose caches capable of storage of any data type within size restrictions imposed by the cache.

Finally, memory object caches can be deployed as single purpose servers or also co-located with another deployment. Consequently, general purpose object caches often provide multiple protocols for accessing the cache - socket communication or TCP over the network. Both caches in question - Memcached and Redis - support both deployment strategies. Our primary focus will be on networked protocols used to access the cache.


\subsection{Example}
TODO: Example usage of a cache for a web server?.


\subsection{Desired qualities}
Firstly, an object cache should support a simple interface providing the following operations - \textit{get}, \textit{set} and \textit{delete} to retrieve, store and invalidate an entry respectively.

Secondly, a general purpose object cache should have the capability to store items of arbitrary format and size provided the size satisfies the upper bound size constraints imposed by the cache. Making no distinction between the type of data is a fundamental generalization of an object cache and allows a greater degree of interoperability.

Thirdly, a cache should support operation atomicity in order to prevent data corruption resulting from multiple simultaneous writes.

Furthermore, cache operations should be performed efficiently, ideally in constant time and the cache should be capable of enforcing a consistent eviction policy in the case of memory bounds are exceeded.

Finally, a general purpose object cache should be capable of handling a large number of requests per second while maintaining a fair and as low as possible quality of service for all connected clients.


\subsection{Design and Implementations}
The design and implementation of a general purpose cache system is heavily influenced by the desired qualities of a cache.

Firstly, high performance requirement and the need for storage of entries of varying size generally requires the cache system to implement custom memory management models. As a result, a mapping data structure  with key hashing is used to efficiently locate entries in the cache.

Secondly, due to memory restrictions, the cache is responsible for enforcing an eviction policy. Most state of the art caches utilize least recently used (LRU) cache eviction policy, however, other policies such as first-in-first-out can also be used.

In the case of \textit{Memcached}, multi-threaded approach is utilized in order to improve performance. Conversely to Memcached, \textit{Redis} is implemented as a single threaded application and focuses primarily on a fast execution loop rather than parallel computation.


\subsection{Performance metrics}
Firstly, the primary metrics reflecting performance of an in memory object cache are \textit{mean latency}, \textit{99th percentile latency} and \textit{throughput}. Both latency statistics are reflective of the quality of service the cache is delivering to it's clients. Throughput is indicative of the overall load the cache is capable of supporting, however, throughput is tightly related to latency and on it's own is not indicative of the real cache performance under quality constraints.

Secondly, being a high performance application with potentially network, understanding the proportion of CPU time spent inside the cache application compared to time spent processing network requests and handling operating system calls becomes important. Having an insight into the CPU time breakdown allows us to better understand bottlenecks of the application.

Finally, the \textit{hit} and \textit{miss} rate of the cache can be used as a metric, particularly when evaluating a cache eviction policy, however, the hit and miss rate is tightly correlated with the type of application and the application context and therefore it is not a suitable metric for evaluating performance alone.


\subsection{Current State of caches and Usage in industry}
TODO


\subsection{Memcached}

Memcached is a ``high-performance, distributed memory object caching system, generic in nature, but intended for use in speeding up dynamic web applications by alleviating database load.'' \cite{interactive2006memcached} Despite the official description aimed at dynamic web applications, memcached is also used as a generic key value store to locate servers and services \cite{atikoglu2012workload}.

\subsubsection{Memcached API}
Memcached provides a simple communication protocol. It implements the following core operations:

\begin{itemize}
    \item \texttt{get key1 [key2..N]} - Retrieve one or more values for given keys,
    \item \texttt{set key value [flag] [expiration] [size]} - Insert \textit{key} into the cache with a \textit{value}. Overwrites current item.
    \item \texttt{delete key} - Delete a given key.
\end{itemize}

Memcached further implements additional useful operations such as \texttt{incr/decr} which increments or decrements a value and \texttt{append/prepend} which append or prepend a given key.

\subsubsection{Implementation}
Firstly, Memcached is implemented as a multi-threaded application. ``Memcache instance started with n threads will spawn n + 1 threads of which the first n are worker threads and the last
is a maintenance thread used for hash table expansion under high load factor.'' \cite{solarflarememcached}

Secondly, in order to provide performance as well as  portability, memcached is implemented on top of \textit{libevent} \cite{libevent}. ``The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts.'' \cite{libevent}

Thirdly, Memcached provides guarantees on the order of actions performed. Therefore, consecutive writes of the same key will result in the last incoming request being the retained by memcached. Consequently, all actions performed are internally atomic.

As a result, memcached employs a locking mechanism in order to be able to guarantee order of writes as well as execute concurrently. Internally, the process of handling a request is as follows:

\begin{enumerate}
    \item Requests are received by the Network Interface Controller (NIC) and queued
    \item \emph{Libevent} receives the request and delivers it to the memcached application
    \item A worker thread receives a request, parses it and determines the command required
    \item The \emph{key} in the request is used to calculate a hash value to access the memory location in \emph{O(1)}
    \item Cache lock is acquired \emph{(entering critical section)}
    \item Command is processed and LRU policy is enforced
    \item Cache lock is released \emph{(leaving critical section)}
    \item Response is constructed and transmitted \cite{wiggins2012enhancing}
\end{enumerate}

We can observe that steps \textit{1-4} and \textit{8} can be parallelized without the need for resource locking. However, the critical section in steps \textit{5-7} is executed with the acquisition of a global lock. Therefore, at this stage execute is not being performed in parallel.


\subsubsection{Production deployments}
TODO: Facebook deployment?

% \subsection{Memcached}
% From the early development stages, memcached has been designed in a client-server architecture. Therefore, a memcached applications receives a command based on its API, executes the command and returns a reply to the client. Memcached is deliberetly designed as a standalone application rather than being integrated into a particular system/framework in order to be able to act as a general purpose cache and allow decoupling of responsibilities in an system architecture.

% Memcached implements its distributed protocol through consistent hashing on the client side. Therefore, keeping logic on the server side minimal and allowing the clients to figure out which instance to talk to. In order to further improve horizontal scaling properties of memcached, solutions such as Twemproxy [9] exist to support scalability of an individual shard of a distributed memcached deployment.

% The memory requirements of memcached are specified as a configration option before the memcached application is started. Knowing an upper bound on the amount of memory memcached can use allows memcached to claim the required memory and handle memory management itself rather than using *malloc, free or realloc*. *Slabs* are structured to be blocks of memory with 1MB allocated to them. Each slab belogns to a *slab group* which determines the size of each chunk inside the slab. By default, the minimum chunk size is 80 bytes with a hard maximum of 1MB. The growth factor between different slab groups is 1.25. Within each slab group, a Least Recently Used (LRU) eviction policy is employed effectively evicting entries least recently used within a similar memory requirement first.

% Private networks are the intended target of memcached where applications designed to be publicly exposed access memcached on behalf of the requestor rather than exposing the cache directly. By default, memcached provides access to all entries of the cache for all clients but there is also an option to be used with a Simple Authentication and Security Layer (SASL) option.

% Memcached is a multi-threaded application which introduces the requirement to lock resources during critical sections in order to prevent race conditions. The main reason for designing multi-threaded applications is improve performance. Parsing a request and understanding the nature of a request can be in parallel with data retrieval from memory while a response is being constructed, all in their own respective threads. However, in order to achieve the apprearance of operation atomicity, a mutual exclusion lock is required. A request lifecycle is as follows [10]:

% \begin{enumerate}
%     \item Requests are received by the Network Interface Controller (NIC) and queued
%     \item \emph{Libevent} receives the request and delivers it to the memcached application
%     \item A worker thread receives a request, parses it and determines the command required
%     \item The \emph{key} in the request is used to calculate a hash value to access the memory location in \emph{O(1)}
%     \item Cache lock is acquired \emph{(entering critical section)}
%     \item Command is processed and LRU policy is enforced
%     \item Cache lock is released \emph{(leaving critical section)}
%     \item Response is constructed and transmitted
% \end{enumerate}

% Given the outline above, we can see that steps 5. to 7. transform the parallel nature of processing a request into a serial process. Optimizations to the critical section have been well studied but it should be noted that memcahed suffers from overheads related to global lock acquizition and release.

% \subsection{Outline}
% Memcached is a simple distributed memory object cache [1]. It provides a simple interface to allow systems to store, retrieve and update the contents of the cache. It is developed as an open source project and has been extensively studied in the literature. Often, it is used as an application of choice to benchmark system configurations in terms of network
% throughput, memory allocation policies and more generally to understand how a system performs under stress. Furthermore, it is often used as an application for experiementation and implementation of next generation technology such as the use of a Field Programmable Gate Array (FPGA) [2].

% The API supported by memcached is straightforward. Memcached philosophy is to execute commands against an item of the cache rather than manipulate the cache as a whole. Out of the box, memcached supports the following operations for retrieval: *set, add, replace, append, prepend and cas* [3]. Similarly, memcached supports the following storage commands: *get, gets, delete and incr/decr* [3]. The API is deliberetly designed to be intuitive making the effect of an action predictable. The action labelled *cas* perhaps requires further clarification, however. The full action name is *check and set*, data is stored only if the comparison with current value fails.

% Memcached has grown to be a very popular general purpose cache in the industry. Currently, Facebook is considered to have the largest deployment of memcached in production [4] while there are many other companies utilizing large deployments of memcached as building blocks of their infrastructure, these include Twitter[5], Amazon [6] and many others.

% In the simplest memcached deployment, an instance of memcached can be run alongside another application, for example a web server. In such a setup, no network communication is required and the web server can talk to memcached over a local unix socket. This configuration has disadvantages, for example, horizontally scaling the web server would require another instance of the cache to be deployed as well potentially leading decreased cache hit rate.

% More complicated deployments generally utilize a memcached instance running on a seperate host with all instances of, for example, web servers communicating with a single memcached host. The advantage of such a setup is decreased coupling and increased potential for scalability by adding more instances of both web server and memcached.

% In the largest scenarios, such as Facebook, a large number of client applications are talking to a number of memcached clusters responsible for a given type of information. Effectively creating a data layer where any client application can request information from any pool increasing modularity and interoperability of the infrastructure. [Ref?]

% (Diagram to illustrate deployments here?)

% To illustrate the importance and also the size of a memcached deployment, a workload charasterization from Facebook will be used [7]. Figure below illustrates the throughput observed in a Facebook pools deployed over the course of 7 days. We can observe that the total number of requests is close to 1.26 trillion requests over 7 days, this is on average 2.08 million requests a second. The volume itself is large, however, considering Facebook has 1.44 billion active monthly users [8], however, it does demonstrate the scale at which Facebook utilizes memcached and the impact memcached has on ability to scale and handle traffic at Facebook.



% \subsection{Configuration options}
% Memcached provides a convenient command line configration options to tweak the performance of memcached through various parameters, the most important ones are:
% \begin{itemize}
%     \item \emph{-d} runs application in daemon mode
%     \item \emph{-p port} binds application to a port (18080 by default)
%     \item \emph{-m memory} defines how much memory to allocate to memcached.
% \end{itemize}

% Given the host hardware has 8GB memory, 6GB will be allocated to memcached to leave some memory for the underlying operating system. Throughout this paper, mostly options outlined above will be utilized. Where applicable, further settings will be explained.
