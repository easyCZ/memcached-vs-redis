\section{Methodology}

In order to effectively benchmark the performance of both types of caches in question, it is essential to be able to stress the cache server sufficiently to experience queuing delay and saturate the server. This study is concerned with the performance of the cache server rather than performance of the underlying network and therefore it is essential to utilize a sufficient number of clients in order to saturate the server while maintaining low congestion on the underlying network.

The benchmarking methodology is heavily influenced by similar studies and benchmarks in the literature. This allows for a comparison of observed results and allows for a better correlation with related research.


\subsection{Quality of Service}
Firstly, it is important the desired quality of service we are looking to benchmark for. Frequently, distributed systems are designed to work in parallel, each component responsible for a piece of computation which is then ultimately assembled into a larger piece of response before being shipped to the client. For example, an e-commerce store may choose to compute suggested products as well as brand new products separately only to assemble individual responses into an HTML page. Therefore, the slowest of all individual components will determine the overall time required to render a response.

Let us define the quality of service (QoS) target of this study. For our benchmarking purposes, a sufficient QoS will be the \textit{99th percentile} tail latency of a system under 1 \textit{millisecond}. This is a reasonable target as the mean latency will generally (based on latency distribution) be significantly smaller. Furthermore, it is a similar latency target used in related research \cite{leverich2014reconciling}.

\subsection{Hardware}
Performance benchmarks executed in this study will be run on 8 distinct machines with the following configuration: \textit{6 core Intel(R) Xeon(R) CPU E5-2603 v3 @ 1.60GHz}, 8 GB RAM and 1Gb/s Network Interface Controller (NIC).

All the hosts are connected to a Pica8 P-3297 [3] switch with 48 1Gbps ports with a star as the network topology. A single host is used to run an object cache system while the remaining seven are used to generate workloads against the server.

\subsection{Workload generation}
Workload for the cache server is generated using Memtier Benchmark [4]. The Memtier Benchmark provides a configurable parallel workload generation for both Memcached and Redis. Additionally, it allows for a high level of configrability.

\subsubsection{Memtier Benchmark Behavior}
Memtier Benchmark provides various paramters allowing for a variable configuration. As part of the configration, the user is allowed to specify the number of threads and the number of connections per each thread memtier should make. The standard lifecycle of each thread is as follows:

1. Set up n connection configurations
2. For each connection configuration, initiate the connection over the desired protocol (default: TCP)
3. Make a request
4. Tear down the connection
5. Repeat iterations

\subsubsection{Open loop vs Closed loop}
Memtier is closed loop



% * Quality of Service (99th < 1ms?)
% * Testing setup
% * Comparison to testbeds in other papers
% * Memtier and comparison to others
% * Justify Memtier
% * Open loop vs closed loop
% * What are we interested in
% * Discuss error conditions and repeatability

% * [1] Reconciling High Server Utilization
% and Sub-millisecond Quality-of-Service
% * [2] [Intel(R) Xeon(R) E5-2603](http://ark.intel.com/products/64592/Intel-Xeon-Processor-E5-2603-10M-Cache-1_80-GHz-6_40-GTs-Intel-QPI)
% * [3] [Pica8 Datasheet](http://www.pica8.com/wp-content/uploads/2015/09/pica8-datasheet-48x1gbe-p3297.pdf)
% * [4] [Memtier Benchmark](https://github.com/RedisLabs/memtier_benchmark)
