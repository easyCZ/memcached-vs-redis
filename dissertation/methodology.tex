\section{Methodology}

In order to effectively benchmark the performance of both types of caches in question, it is essential to be able to stress the cache server sufficiently to experience queuing delay and saturate the server. This study is concerned with the performance of the cache server rather than performance of the underlying network and therefore it is essential to utilize a sufficient number of clients in order to saturate the server while maintaining low congestion on the underlying network.

The benchmarking methodology is heavily influenced by similar studies and benchmarks in the literature. This allows for a comparison of observed results and allows for a better correlation with related research.


\subsection{Quality of Service}
Firstly, it is important the desired quality of service we are looking to benchmark for. Frequently, distributed systems are designed to work in parallel, each component responsible for a piece of computation which is then ultimately assembled into a larger piece of response before being shipped to the client. For example, an e-commerce store may choose to compute suggested products as well as brand new products separately only to assemble individual responses into an HTML page. Therefore, the slowest of all individual components will determine the overall time required to render a response.

Let us define the quality of service (QoS) target of this study. For our benchmarking purposes, a sufficient QoS will be the \textit{99th percentile} tail latency of a system under 1 \textit{millisecond}. This is a reasonable target as the mean latency will generally (based on latency distribution) be significantly smaller. Furthermore, it is a similar latency target used in related research \cite{leverich2014reconciling}.

\subsection{Hardware}
Performance benchmarks executed in this study will be run on 8 distinct machines with the following configuration: \textit{6 core Intel(R) Xeon(R) CPU E5-2603 v3 @ 1.60GHz}, \textit{8 GB RAM} and \textit{1Gb/s Network Interface Controller} (NIC).

All the hosts are connected to a \textit{Pica8 P-3297} switch with 48 1Gbps ports arranged in a star topology. A single host is used to run an object cache system while the remaining seven are used to generate workloads against the server.

\subsection{Workload generation}
\label{methodology:workload-gen}
Workload for the cache server is generated using Memtier Benchmark developed by Redis Labs \cite{memtier}. Memtier has been chosen as the benchmark for this study due to it's high level of configurability as well as ability to benchmark both \textit{Memcached} and \textit{Redis}. Utilizing the same benchmark client for both caches allows for a decreased variability in results when a comparison is made.

In order to create a more realistic simulation of a given workload, 7 servers all running \textit{memtier} simultaneously are used. A simple parallel ssh utility is used to start, stop and collect statistics from the load generating clients.

The workload generated by Memtier is driven by the configuration specified. The keys and values are drawn from a configured distribution dynamically at runtime. All comparable benchmarks presented in this thesis configure the same initial seed for comparable benchmarks in order to minimize stochastic behavior.

\subsection{Benchmark}

In the context of this thesis, a benchmark is a set of workloads executed against the cache host. Statistics are collected from the cache host as well as the clients in order to draw conclusions.

Firstly, a benchmark consists of a warm up stage. The cache is being loaded with initial data in order to prevent a large number of cache misses and skewed results.

Secondly, a configured workload is generated and issued against the cache host from multiple benchmarking hosts simultaneously.

Thirdly, the workload is repeated 2 more times in order to decrease the impact of stochastic events in the benchmark.

Finally, statistics are collected, individual benchmark runs are averaged and the results are processed.



\subsubsection{Memtier}
Memtier benchmark is ``a command line utility developed by Redis Labs for load generation and benchmarking NoSQL key-value databases'' \cite{memtier}. It provides a high level of configurability allowing for example to specify patterns of \textit{sets} and \textit{gets} as well as generation of key-value pairs according to various distributions, including Gaussian and pseudo-random.

Memtier is a threaded application built on top of \texttt{libevent} \cite{libevent}, allowing the user to configure the number of threads as well as the number of connections per each thread which can be used to control the server load. Additionally, memtier collects benchmark statistics including latency distribution, throughput and mean latency. The statistics reported are used to draw conclusions on the performance under a given load.

Memtier execution model is based on the number of threads and connections configured. For each thread \texttt{t}, there are \texttt{c} connections created. The execution pattern within each thread is as follows:

\begin{enumerate}
    \item Initiate \texttt{c} connections
    \item For each connection
        \begin{enumerate}
            \item Make a request to the cache server
            \item Provide a \textit{libevent} callback to handle response outside of the main event loop
        \end{enumerate}
    \item Tear down \texttt{c} connections
\end{enumerate}

By offloading response handling to a callback inside libevent, memtier is able to process a large number of requests without blocking the main event loop until a response from the network request is returned while maintaining the ability to collect statistics effectively.

Connections created with the target server are only destroyed at the end of the benchmark. This is a realistic scenario as in a large distributed environment the cache clients will maintain open connections to the cache to reduce the overhead of establishing a connection.

Memtier provides a comprehensive set of configuration options to customize Memtier behavior and tailor the load. Table \ref{tab:memtier_config} outlines the relevant configuration options. The complete set of configuration options is available on RedisLabs \cite{MemtierConfiguration}.

\begin{table}[h!]
\centering
\begin{tabular}{| c c c |}
 \hline
 Configuration option & Explanation & Default Value\\ [0.5ex]
 \hline\hline

 -s & Server Address & localhost \\
 -p & Port number & 6379 \\
 -P & Protocol - redis, memcache\_text, memcache\_binary & redis \\
 -c & Number of Clients per Thread & 50 \\
 -t & Number of Threads & 4 \\
 --data-size & The size of the object to send in bytes & 32 \\
 --random-data & Data should be randomized & false \\
 --key-minimum & The minimum value of keys to generate & 0 \\
 --key-maximum & The maximum value of keys to generate & 10 million \\

 \hline

\end{tabular}
\caption{Memtier Configuration Options}
\label{tab:memtier_config}
\end{table}


\subsubsection{Open-loop vs Closed-loop}
A load tester can be constructed with different architecture in mind. The main two types of load testers are \textit{open-loop} and \textit{closed-loop}. Closed-loop load testers frequently construct and send a new request only when the previous request has received a response. On the other hand, open-loop principle aims to send requests in timed intervals regardless of the response from the previous requests.

The consequence of a closed-loop load tester is potentially reduced queuing on the server side and therefore observed latency distribution may be lower than when server side queuing is observed.

Memtier falls in the category of closed loop testers when considering a single thread of memtier. However, memtier threads are independent of each other and therefore requests for another connection are made even if the previous request has not responded. Furthermore, by running memtier on multiple hosts simultaneously, the closed loop implications are alleviated and the server observes queuing delay in the network stack.
